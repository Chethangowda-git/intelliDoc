# IntelliDoc

**AI-powered document intelligence platform with RAG pipeline**

Upload a PDF or DOCX, ask questions in natural language, and get answers grounded in your document â€” with source citations. Supports both open-source (HuggingFace) and proprietary (Gemini) model providers, switchable at runtime.

---

## Demo

> Upload a document â†’ watch it process in real-time â†’ ask anything â†’ get a streamed answer with citations

**Core demo moments:**
- Upload a PDF and watch the status cycle: `Queued â†’ Extracting â†’ Embedding â†’ Ready`
- Switch between ğŸ¤— HuggingFace and âœ¨ Gemini using the header toggle
- Ask a question and watch the answer stream token by token
- See source citations below each answer showing exactly which chunks were used

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INGESTION PIPELINE                    â”‚
â”‚                                                             â”‚
â”‚  Upload â”€â”€â–º Fastify â”€â”€â–º Save to disk â”€â”€â–º MongoDB (queued)   â”‚
â”‚                                â†“                            â”‚
â”‚                          BullMQ Queue                       â”‚
â”‚                                â†“                            â”‚
â”‚                         Worker picks up                     â”‚
â”‚                                â†“                            â”‚
â”‚            pdf-parse / mammoth â”€â”€â–º Raw text                 â”‚
â”‚                                â†“                            â”‚
â”‚                    Chunker (500 chars, 50 overlap)           â”‚
â”‚                                â†“                            â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚         â–¼                                              â–¼    â”‚
â”‚  HF all-MiniLM-L6-v2                    Gemini embedding-001â”‚
â”‚  (384 dims)                             (3072 dims)         â”‚
â”‚         â†“                                              â†“    â”‚
â”‚  ChromaDB: doc_{id}_huggingface  ChromaDB: doc_{id}_gemini  â”‚
â”‚                                                             â”‚
â”‚                    File deleted from disk                   â”‚
â”‚                    MongoDB: status â†’ ready                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         QUERY PIPELINE                       â”‚
â”‚                                                             â”‚
â”‚  User question                                              â”‚
â”‚        â†“                                                    â”‚
â”‚  Embed with selected provider (HF or Gemini)                â”‚
â”‚        â†“                                                    â”‚
â”‚  ChromaDB cosine similarity search (HNSW)                   â”‚
â”‚        â†“                                                    â”‚
â”‚  Top 5 semantically relevant chunks                         â”‚
â”‚        â†“                                                    â”‚
â”‚  Prompt: "Use ONLY this context to answer..."               â”‚
â”‚        â†“                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Qwen2.5-7B (HF) â”‚   or    â”‚ gemini-2.5-flash        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚        â†“                                                    â”‚
â”‚  Streamed tokens via SSE â”€â”€â–º Frontend                       â”‚
â”‚        â†“                                                    â”‚
â”‚  Save to MongoDB Conversation                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Tech Stack

| Layer | Technology | Why |
|---|---|---|
| **Backend** | Fastify (Node.js) | Faster than Express, built-in schema validation |
| **Frontend** | Vite + React | Fast dev server, no CRA bloat |
| **Job Queue** | BullMQ + Redis | Async processing â€” upload returns instantly, processing happens in background |
| **Vector DB** | ChromaDB | Local vector storage with cosine similarity search, no cloud needed |
| **Document DB** | MongoDB | Flexible schema for documents and conversation history |
| **Embeddings (OSS)** | HuggingFace all-MiniLM-L6-v2 | Fast, free, 384-dim, runs via HF Inference API |
| **Chat (OSS)** | Qwen2.5-7B-Instruct | Strong open-source model, available on HF free tier |
| **Embeddings (API)** | Gemini gemini-embedding-001 | 3072-dim embeddings, high quality |
| **Chat (API)** | Gemini 2.5 Flash | Fast, low cost, streaming support |
| **Streaming** | Server-Sent Events (SSE) | Real-time token streaming without WebSocket overhead |
| **Auth** | JWT | Stateless, works with any client |
| **Infra** | Docker Compose | One-command local setup for all services |

---

## Why RAG and not just "send the document to the LLM"?

Three reasons:

**1. Context window limits** â€” A 50-page PDF is ~100,000 tokens. Most models cap at 8kâ€“32k tokens in practice. RAG retrieves only the 5 most relevant chunks (~2,500 tokens) regardless of document size.

**2. Cost** â€” Sending a full document on every query costs 40x more tokens than RAG retrieval. At scale this matters.

**3. Precision** â€” The model is forced to answer from retrieved chunks only. The system prompt says `"Use ONLY the context below"` â€” this grounds the answer in your document and prevents hallucination from training data.

The retrieval step uses **cosine similarity** between embedding vectors. When you embed `"invoice due within 30 days"` and later embed `"what are the payment terms?"`, both vectors land close together in 384-dimensional space because they share semantic meaning â€” even though they share zero keywords.

---

## Why Two Embedding Providers?

Both collections are built during ingestion so provider switching at query time is instant â€” no re-processing needed.

The embedding model must match between ingestion and query. If you embed documents with HuggingFace but query with Gemini, the vectors live in different mathematical spaces and similarity search returns garbage. This is why `doc_{id}_huggingface` and `doc_{id}_gemini` are separate ChromaDB collections.

---

## Project Structure

```
intellidoc/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â”œâ”€â”€ db.js              # MongoDB connection
â”‚   â”‚   â”‚   â”œâ”€â”€ redis.js           # Redis + BullMQ connections
â”‚   â”‚   â”‚   â”œâ”€â”€ chroma.js          # ChromaDB client + collection helpers
â”‚   â”‚   â”‚   â”œâ”€â”€ gemini.js          # Gemini embed + chat
â”‚   â”‚   â”‚   â””â”€â”€ huggingface.js     # HF embed + chat
â”‚   â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”‚   â””â”€â”€ auth.js            # JWT verification
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ User.js            # User schema
â”‚   â”‚   â”‚   â”œâ”€â”€ Document.js        # Document schema + status enum
â”‚   â”‚   â”‚   â””â”€â”€ Conversation.js    # Chat history schema
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.js            # Register + login
â”‚   â”‚   â”‚   â”œâ”€â”€ documents.js       # Upload + list + delete
â”‚   â”‚   â”‚   â””â”€â”€ chat.js            # RAG Q&A with SSE streaming
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”œâ”€â”€ textExtractor.js   # pdf-parse + mammoth
â”‚   â”‚   â”‚   â””â”€â”€ chunker.js         # Fixed-size chunking with overlap
â”‚   â”‚   â””â”€â”€ workers/
â”‚   â”‚       â””â”€â”€ documentWorker.js  # BullMQ worker â€” extract, chunk, embed
â”‚   â”œâ”€â”€ .env
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ client.js          # Axios instance + JWT interceptor
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatPanel.jsx      # SSE streaming chat UI
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentCard.jsx   # Status badge + click to chat
â”‚   â”‚   â”‚   â”œâ”€â”€ EmptyState.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ SkeletonCard.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Toast.jsx
â”‚   â”‚   â”‚   â””â”€â”€ UploadZone.jsx     # react-dropzone
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â””â”€â”€ useToast.js
â”‚   â”‚   â””â”€â”€ pages/
â”‚   â”‚       â”œâ”€â”€ AuthPage.jsx       # Register + login
â”‚   â”‚       â””â”€â”€ DashboardPage.jsx  # Main app shell + provider toggle
â”‚   â””â”€â”€ package.json
â””â”€â”€ docker-compose.yml
```

---

## Local Setup

### Prerequisites

- Node.js 18+
- Docker + Docker Compose
- HuggingFace API key (free) â€” [get one here](https://huggingface.co/settings/tokens)
- Gemini API key (free tier) â€” [get one here](https://aistudio.google.com/apikey)

### 1. Clone and install

```bash
git clone https://github.com/yourusername/intellidoc.git
cd intellidoc

# Backend
cd backend && npm install

# Frontend
cd ../frontend && npm install
```

### 2. Configure environment

```bash
cp backend/.env.example backend/.env
```

Edit `backend/.env`:

```env
PORT=3001
MONGODB_URI=mongodb://localhost:27017/intellidoc
REDIS_URL=redis://localhost:6379
JWT_SECRET=your-secret-key-change-this
CHROMA_URL=http://localhost:8000
HF_API_KEY=hf_your_key_here
GEMINI_API_KEY=your_gemini_key_here
```

### 3. Start infrastructure

```bash
docker compose up -d
```

This starts MongoDB, Redis, and ChromaDB locally.

### 4. Start backend + frontend

```bash
# Terminal 1
cd backend && npm run dev

# Terminal 2
cd frontend && npm run dev
```

Open **http://localhost:5173**

---

## Key Technical Decisions

### Why BullMQ instead of processing synchronously?

A large PDF can take 30â€“60 seconds to extract, chunk, and embed. Processing synchronously inside the HTTP handler would time out the upload request and block the server. BullMQ queues the job and returns the upload response instantly â€” the worker processes in the background and MongoDB status updates drive the UI polling.

### Why fixed-size chunking with overlap?

Semantic chunking (splitting on paragraph/sentence boundaries) produces more natural chunks but adds significant complexity. Fixed-size chunking with 50-character overlap is simpler and effective â€” the overlap ensures sentences that straddle a chunk boundary appear in both adjacent chunks, so no context is lost at edges.

### Why ChromaDB instead of MongoDB for vectors?

MongoDB stores documents. ChromaDB stores vectors and runs cosine similarity search efficiently using HNSW (Hierarchical Navigable Small World) indexing. HNSW finds nearest vectors in O(log n) instead of O(n) â€” irrelevant at 36 chunks but critical at millions. Using MongoDB for vector search would require full collection scans.

### Why SSE instead of WebSockets for streaming?

SSE is one-way (server â†’ client), which is all that's needed for token streaming. WebSockets are bidirectional and add handshake overhead. SSE works over standard HTTP, is simpler to implement, and works through proxies and load balancers without special configuration.

### Why separate ChromaDB collections per provider?

Embedding vectors from different models are not comparable. `all-MiniLM-L6-v2` produces 384-dimensional vectors; `gemini-embedding-001` produces 3072-dimensional vectors. Even if dimensions matched, the vector spaces are different â€” similarity scores would be meaningless. Separate collections guarantee queries always use the correct vector space.

---

## Known Limitations

- **No OCR support** â€” scanned PDFs (image-based) return empty text. Only text-layer PDFs work.
- **Fixed chunking** â€” semantic chunking would produce better retrieval for documents with clear section boundaries.
- **Single document chat** â€” conversations are per-document. Cross-document Q&A is not supported.
- **HF free tier rate limits** â€” the HuggingFace Inference API has per-minute limits on the free tier. Heavy testing will hit these.
- **Local ChromaDB** â€” data is stored in a Docker volume. If the volume is deleted, all embeddings must be regenerated by re-uploading documents.

## What I Would Add in v2

- Semantic chunking using sentence boundaries
- OCR via Tesseract for scanned documents
- Cross-document Q&A â€” query across multiple uploaded documents simultaneously
- Streaming for HuggingFace (currently simulated word-by-word)
- Ollama support for fully offline local inference
- Pinecone integration for persistent cloud vector storage
